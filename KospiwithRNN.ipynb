{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2026\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/kospi.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors = 'coerce') # 날짜 형식으로 변환\n",
    "    df = df.sort_values('Date').reset_index(drop=True) # 날짜 타입으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=2026):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_seq(X, y, L):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - L): # L = 20일을 줄것임.\n",
    "        Xs.append(X[i:i+L]) # 예측하는데에 필요한 기간\n",
    "        ys.append(y[i+L]) # 이후 예측\n",
    "    Xs = torch.tensor(np.array(Xs), dtype=torch.float32)\n",
    "    ys = torch.tensor(np.array(ys), dtype=torch.float32).view(-1, 1)\n",
    "    return Xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_after_split(train_df, test_df, cols): # train과 test로 나눈 후에 결측치를 채우는 함수\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    train_df[cols] = train_df[cols].ffill() # 과거값으로 ffill\n",
    "    bridge = pd.concat([train_df.tail(1), test_df], axis = 0) # train과 test의 경계값을 연결\n",
    "    # train 마지막행 + test를 이어붙여 ffill후 test만 분리\n",
    "    bridge[cols] = bridge[cols].ffill() # 연결된 데이터에서 ffill\n",
    "    test_df[cols] = bridge.iloc[1:][cols].values # test_df에 채워진 값 적용\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 30, min_delta = 1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.count = 0\n",
    "        self.best = None\n",
    "        self.best_state = None\n",
    "    def step(self, val_loss, model):\n",
    "        if self.best is None or val_loss < self.best - self.min_delta:\n",
    "            self.best = val_loss\n",
    "            self.count = 0\n",
    "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.count += 1\n",
    "        return self.count >= self.patience\n",
    "\n",
    "    def load_best(self, model, device):\n",
    "        model.load_state_dict({k: v.to(device) for k, v in self.best_state.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 64, num_layers = 2, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers\n",
    "                          , batch_first = True, nonlinearity= 'tanh', dropout = dropout if num_layers > 1 else 0.0)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(hidden_size), nn.Linear(hidden_size, 64), nn.ReLU(),\n",
    "                                  nn.Dropout(dropout), nn.Linear(64, 1))\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        h0 = torch.zeros(self.rnn.num_layers, B, self.rnn.hidden_size, device = x.device)\n",
    "        out, _ = self.rnn(x, h0) #out : (B, L, hidden_size)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2026)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "csv_path = \"./dataset/kospi.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    " \n",
    "FEATS = ['Open', 'High', 'Low', 'Close']\n",
    "TARGET = ['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.3\n",
    "test_start = int(len(df) * (1 - test_ratio))\n",
    "trainval_df = df.iloc[:test_start].copy()\n",
    "test_df = df.iloc[test_start:].copy()\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_start = int(len(trainval_df) * (1 - val_ratio))\n",
    "train_df = trainval_df.iloc[:val_start].copy()\n",
    "val_df = trainval_df.iloc[val_start:].copy()\n",
    "train_df, val_df = fill_missing_after_split(train_df, val_df,  cols = FEATS)\n",
    "trainval_df, test_df = fill_missing_after_split(trainval_df, test_df, cols = FEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(train_df[FEATS].values)\n",
    "X_val = scaler_x.transform(val_df[FEATS].values)\n",
    "X_test = scaler_x.transform(test_df[FEATS].values)\n",
    "X_train\n",
    "# array([[0.23348791, 0.2330902 , 0.22130596, 0.22017413],\n",
    "#        [0.12607878, 0.13132982, 0.12446595, 0.12910621],\n",
    "#        [0.11628783, 0.1221546 , 0.11416516, 0.12365438],\n",
    "#        ...,\n",
    "#        [0.21824269, 0.21755404, 0.2196509 , 0.21625878],\n",
    "#        [0.21816626, 0.21748705, 0.21687001, 0.21420059],\n",
    "#        [0.21197648, 0.21184884, 0.21360321, 0.2108979 ]], shape=(2527, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler_y.fit_transform(train_df[TARGET].values)\n",
    "y_val = scaler_y.transform(val_df[TARGET].values)\n",
    "y_test = scaler_y.transform(test_df[TARGET].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 20\n",
    "X_train_seq, y_train_seq = make_seq(X_train, y_train, L)\n",
    "X_val_seq, y_val_seq = make_seq(X_val, y_val, L)\n",
    "X_test_seq, y_test_seq = make_seq(X_test, y_test, L)\n",
    "print(X_train_seq.shape, y_train_seq.shape)\n",
    "print(X_val_seq.shape, X_val_seq.shape)\n",
    "print(X_test_seq.shape, X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_seq, y_train_seq), batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_seq, y_val_seq), batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_seq, y_test_seq), batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNRegressor(input_size = X_train_seq.size(2), hidden_size=64, num_layers = 2, dropout = 0.1).to(device)  \n",
    "model\n",
    "\n",
    "# RNNRegressor(\n",
    "#   (rnn): RNN(4, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
    "#   (head): Sequential(\n",
    "#     (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "#     (1): Linear(in_features=64, out_features=64, bias=True)\n",
    "#     (2): ReLU()\n",
    "#     (3): Dropout(p=0.1, inplace=False)\n",
    "#     (4): Linear(in_features=64, out_features=1, bias=True)\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 2e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_norm = 1.0\n",
    "epochs = 300\n",
    "early = EarlyStopping(patience=30, min_delta=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # clip_norm: 모든 파라미터의 기울기를 모아서, 크기가 max_norm을 넘으면 강제로 줄임\n",
    "            # 예) L2 norm 계산해서 현재 norm = 5.0 라면 max_norm이 1.0 일 때 모든 gradient에 1/5를 곱함\n",
    "            # 0.5 ~ 5.0 사이가 안정적\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = run_epoch(train_loader, train=True)\n",
    "    va_loss = run_epoch(val_loader, train=False)\n",
    "    scheduler.step(va_loss)\n",
    "    if epoch == 1 or epoch % 20 == 0:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"[{epoch:03d}/{epochs}] train MSE: {tr_loss:.6f} | val MSE: {va_loss:.6f} | lr: {lr:.2e}\")\n",
    "    if early.step(va_loss, model):\n",
    "        print(f'Early stop at epoch {epoch}. BEST val MSE: {early.best:.6f}')\n",
    "        break\n",
    "early.load_best(model, device)\n",
    "\n",
    "# [001/300] train MSE: 0.022115 | val MSE: 0.011054 | lr: 2.00e-03\n",
    "# [020/300] train MSE: 0.003933 | val MSE: 0.014996 | lr: 1.00e-03\n",
    "# Early stop at epoch 31. BEST val MSE: 0.011054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_list, true_list = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        pred_list.append(pred.cpu().numpy())\n",
    "        true_list.append(yb.cpu().numpy())\n",
    "pred_scaled = np.vstack(pred_list) # 배치 단위로 예측한 결과를 하나의 배열로 합침\n",
    "true_scaled = np.vstack(true_list)\n",
    "\n",
    "pred_unscaled = scaler_y.inverse_transform(pred_scaled) # 예측값을 원래 스케일로 복원\n",
    "true_unscaled = scaler_y.inverse_transform(true_scaled)\n",
    "mse = mean_squared_error(true_unscaled, pred_unscaled)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_unscaled, pred_unscaled)\n",
    "r2 = r2_score(true_unscaled, pred_unscaled)\n",
    "print(f\"Test MSE: {mse:.6f} | RMSE: {rmse:.6f} | MAE: {mae:.6f} | R2: {r2:.6f}\")\n",
    "\n",
    "# Test MSE: 19748.472656 | RMSE: 140.529259 | MAE: 127.858833 | R2: 0.571647\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t0 = test_start\n",
    "N_pred = len(pred_unscaled) #1334: 예측값의 개수\n",
    "y_actual_test = df['Close'].values[t0 + L : t0 + L + N_pred] # 실제값 (L=20일 이후부터 예측값 개수만큼)\n",
    "y_pred_test = pred_unscaled.ravel() # 예측값을 1차원 배열로 변환\n",
    "x_test = np.arange(t0 + L, t0 + L + N_pred)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(df[\"Close\"].values, alpha=0.25, label=\"full series\")\n",
    "plt.plot(x_test, y_actual_test, label=\"actual (test)\")\n",
    "plt.plot(x_test, y_pred_test, linewidth=0.9, label=\"prediction\")\n",
    "plt.legend()\n",
    "plt.title(\"KOSPI Close — Test segment (aligned)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
