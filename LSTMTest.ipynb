{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae0bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt'\n",
    "data = pd.read_table(url, names=['rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eaca356",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'] != 3]\n",
    "data['label'] = np.where(data['rating'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c54f82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7babc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab(dicpath='C:/mecab/mecab-ko-dic')\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를']\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = mecab.morphs(text)\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "data['tokenized'] = data['review'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8240dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for tokens in data['tokenized'] for token in tokens]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab_size = len(vocab) + 2  # 패딩(0), OOV(1) 고려\n",
    "\n",
    "word_to_index = {word: idx + 2 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<OOV>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f740e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [word_to_index.get(token, 1) for token in tokens]\n",
    "\n",
    "data['encoded'] = data['tokenized'].apply(encode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582edca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
    "\n",
    "data['padded'] = data['encoded'].apply(lambda x: pad_sequence(x, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f0a3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = torch.tensor(reviews, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7991f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['padded'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539aa095",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e04c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # 배치 정규화 추가\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.batch_norm(lstm_out[:, -1, :])  # 배치 정규화 적용\n",
    "        out = self.fc(out)\n",
    "        return out  # BCEWithLogitsLoss 내부에서 sigmoid 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec364619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(41130, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 512  # 은닉 차원 증가\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# SentimentLSTM(\n",
    "#   (embedding): Embedding(41130, 128)\n",
    "#   (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
    "#   (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (fc): Linear(in_features=512, out_features=1, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6cbb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1f2276",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, n_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(reviews)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:630\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    622\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    623\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[1;32m--> 630\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SAMSUNG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    867\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for reviews, labels in train_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(reviews).squeeze()\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = (torch.sigmoid(predictions) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_acc = correct / total\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, criterion, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in test_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            predictions = model(reviews).squeeze()\n",
    "            preds = (predictions >= 0.5).float()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            predictions_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789400e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    encoded = encode_tokens(tokens)\n",
    "    padded = pad_sequence(encoded, max_len)\n",
    "\n",
    "    input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor).item()\n",
    "        probability = torch.sigmoid(torch.tensor(prediction)).item()  # 확률로 변환\n",
    "\n",
    "    sentiment = \"긍정\" if probability >= 0.5 else \"부정\"\n",
    "    print(f\"입력 문장: {sentence}\")\n",
    "    print(f\"예측 확률: {probability:.4f} ({sentiment})\")\n",
    "\n",
    "# 테스트\n",
    "test_sentences = [\n",
    "    \"이 제품 정말 좋아요! 추천합니다.\",\n",
    "    \"완전 별로예요. 사지 마세요.\",\n",
    "    \"기대 이하입니다. 실망했어요.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predict_sentiment(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea591559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt'\n",
    "data = pd.read_table(url, names=['rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b728822",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'] != 3]\n",
    "data['label'] = np.where(data['rating'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a68a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab(dicpath='C:/mecab/mecab-ko-dic')\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를']\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = mecab.morphs(text)\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "data['tokenized'] = data['review'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for tokens in data['tokenized'] for token in tokens]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab_size = len(vocab) + 2  # 패딩(0), OOV(1) 고려\n",
    "\n",
    "word_to_index = {word: idx + 2 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<OOV>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a419a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [word_to_index.get(token, 1) for token in tokens]\n",
    "\n",
    "data['encoded'] = data['tokenized'].apply(encode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
    "\n",
    "data['padded'] = data['encoded'].apply(lambda x: pad_sequence(x, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927043f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = torch.tensor(reviews, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed701aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['padded'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # 배치 정규화 추가\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.batch_norm(lstm_out[:, -1, :])  # 배치 정규화 적용\n",
    "        out = self.fc(out)\n",
    "        return out  # BCEWithLogitsLoss 내부에서 sigmoid 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0885b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(41130, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "embedding_dim = 128\n",
    "hidden_dim = 512  # 은닉 차원 증가\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c662f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for reviews, labels in train_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(reviews).squeeze()\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = (torch.sigmoid(predictions) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_acc = correct / total\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, criterion, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in test_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            predictions = model(reviews).squeeze()\n",
    "            preds = (predictions >= 0.5).float()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            predictions_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1aeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    encoded = encode_tokens(tokens)\n",
    "    padded = pad_sequence(encoded, max_len)\n",
    "\n",
    "    input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor).item()\n",
    "        probability = torch.sigmoid(torch.tensor(prediction)).item()  # 확률로 변환\n",
    "\n",
    "    sentiment = \"긍정\" if probability >= 0.5 else \"부정\"\n",
    "    print(f\"입력 문장: {sentence}\")\n",
    "    print(f\"예측 확률: {probability:.4f} ({sentiment})\")\n",
    "\n",
    "# 테스트\n",
    "test_sentences = [\n",
    "    \"이 제품 정말 좋아요! 추천합니다.\",\n",
    "    \"완전 별로예요. 사지 마세요.\",\n",
    "    \"기대 이하입니다. 실망했어요.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predict_sentiment(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83265bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d5445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
