{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc092e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab237456",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt'\n",
    "data = pd.read_table(url, names=['rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbded925",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating'] != 3]\n",
    "data['label'] = np.where(data['rating'] > 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42711",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab(dicpath='C:/mecab/mecab-ko-dic')\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를']\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = mecab.morphs(text)\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "data['tokenized'] = data['review'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca522b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for tokens in data['tokenized'] for token in tokens]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab_size = len(vocab) + 2  # 패딩(0), OOV(1) 고려\n",
    "\n",
    "word_to_index = {word: idx + 2 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<OOV>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c90416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):\n",
    "    return [word_to_index.get(token, 1) for token in tokens]\n",
    "\n",
    "data['encoded'] = data['tokenized'].apply(encode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq[:max_len] + [0] * (max_len - len(seq))\n",
    "\n",
    "data['padded'] = data['encoded'].apply(lambda x: pad_sequence(x, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = torch.tensor(reviews, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971de60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['padded'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(SentimentGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM -> GRU 변경\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)  # 배치 정규화 추가\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)  # GRU 실행\n",
    "        out = self.batch_norm(gru_out[:, -1, :])  # 마지막 타임스텝의 출력 사용\n",
    "        out = self.fc(out)\n",
    "        return out  # BCEWithLogitsLoss 내부에서 sigmoid 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9624f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(41130, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 512  # 은닉 차원 증가\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# SentimentLSTM(\n",
    "#   (embedding): Embedding(41130, 128)\n",
    "#   (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
    "#   (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (fc): Linear(in_features=512, out_features=1, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for reviews, labels in train_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(reviews).squeeze()\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = (torch.sigmoid(predictions) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_acc = correct / total\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, criterion, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in test_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            predictions = model(reviews).squeeze()\n",
    "            preds = (predictions >= 0.5).float()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            predictions_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50498b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    encoded = encode_tokens(tokens)\n",
    "    padded = pad_sequence(encoded, max_len)\n",
    "\n",
    "    input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor).item()\n",
    "        probability = torch.sigmoid(torch.tensor(prediction)).item()  # 확률로 변환\n",
    "\n",
    "    sentiment = \"긍정\" if probability >= 0.5 else \"부정\"\n",
    "    print(f\"입력 문장: {sentence}\")\n",
    "    print(f\"예측 확률: {probability:.4f} ({sentiment})\")\n",
    "\n",
    "# 테스트\n",
    "test_sentences = [\n",
    "    \"이 제품 정말 좋아요! 추천합니다.\",\n",
    "    \"완전 별로예요. 사지 마세요.\",\n",
    "    \"기대 이하입니다. 실망했어요.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predict_sentiment(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c772c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
